{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176622ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------\n",
    "# # Comparison table\n",
    "# # -------------------------\n",
    "# results = []\n",
    "# models = [\"NaiveBayes\",\"LogisticReg\",\"TextCNN\",\"BiLSTM\",\"BERT\",\"DistilBERT\",\"Ensemble\"]\n",
    "# preds_list = [nb_preds, lr_preds, cnn_preds, lstm_preds, preds_bert, preds_distil, ensemble_preds]\n",
    "\n",
    "# for name,preds in zip(models, preds_list):\n",
    "#     results.append({\n",
    "#         \"model\": name,\n",
    "#         \"accuracy\": float(accuracy_score(test_labels, preds)),\n",
    "#         \"f1_spam\": float(f1_score(test_labels, preds, pos_label=1))\n",
    "#     })\n",
    "\n",
    "# df = pd.DataFrame(results).round(4)\n",
    "# print(\"\\n=== Comparison ===\")\n",
    "# print(df)\n",
    "\n",
    "# print(\"\\nDone.\")\n",
    "# print(\"If the dataset load failed, ensure you ran 'huggingface-cli login' in your terminal.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adjusted and robust version of spam_all_models.py\n",
    "Run: python spam_all_models_fixed.py\n",
    "\n",
    "Notes:\n",
    " - Handles various label formats robustly\n",
    " - Fixes undefined variables (train_texts, etc.)\n",
    " - Adds safer HF dataset loading and mapping\n",
    " - Keeps same model set but with clearer data flow and minor training stability tweaks\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# -------------------------\n",
    "# Basic config & seeds\n",
    "# -------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -------------------------\n",
    "# Try loading dataset robustly\n",
    "# -------------------------\n",
    "HF_DATASET = \"mshenoda/spam-messages\"\n",
    "try:\n",
    "    print(f\"Attempting to load dataset: {HF_DATASET}\")\n",
    "    ds = load_dataset(HF_DATASET)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load dataset from HuggingFace:\\n\", e)\n",
    "    print(\"If this is a permission issue, try running: huggingface-cli login\")\n",
    "    raise\n",
    "\n",
    "# Some HF datasets may store label as int or str, and text column may be named differently\n",
    "# Find text column name\n",
    "example_cols = ds['train'].column_names\n",
    "print(\"Dataset columns (train):\", example_cols)\n",
    "\n",
    "# Heuristics for column names\n",
    "if 'text' in example_cols:\n",
    "    text_col = 'text'\n",
    "elif 'message' in example_cols:\n",
    "    text_col = 'message'\n",
    "elif 'sms' in example_cols:\n",
    "    text_col = 'sms'\n",
    "else:\n",
    "    # fallback to first string column\n",
    "    text_col = next((c for c in example_cols if ds['train'].features[c].dtype == 'string'), 'text')\n",
    "print('Using text column:', text_col)\n",
    "\n",
    "# Normalize labels into integers 0 (ham) / 1 (spam)\n",
    "LABEL_TO_ID = {\"ham\": 0, \"spam\": 1, \"0\": 0, \"1\": 1}\n",
    "\n",
    "def normalize_label(example):\n",
    "    lab = example.get('label') if 'label' in example else example.get('Label')\n",
    "    if lab is None:\n",
    "        # Try to infer from other columns\n",
    "        for c in example:\n",
    "            if isinstance(example[c], (int, float)) and example[c] in (0,1):\n",
    "                example['label'] = int(example[c])\n",
    "                return example\n",
    "    # If label is already int-like\n",
    "    try:\n",
    "        example['label'] = int(lab)\n",
    "        return example\n",
    "    except Exception:\n",
    "        # try mapping string\n",
    "        lab_s = str(lab).lower()\n",
    "        if lab_s in LABEL_TO_ID:\n",
    "            example['label'] = LABEL_TO_ID[lab_s]\n",
    "            return example\n",
    "    raise ValueError(f\"Unable to normalize label: {lab}\")\n",
    "\n",
    "# Apply normalization\n",
    "ds = ds.map(normalize_label)\n",
    "\n",
    "# Split aliases\n",
    "train = ds['train']\n",
    "val = ds['validation'] if 'validation' in ds else ds['test'].train_test_split(test_size=0.2, seed=SEED)['train']\n",
    "test = ds['test'] if 'test' in ds else (ds['validation'] if 'validation' in ds else val)\n",
    "\n",
    "# Convert to lists for the classic models\n",
    "train_texts = [t[text_col] for t in train]\n",
    "train_labels = [int(t['label']) for t in train]\n",
    "val_texts = [t[text_col] for t in val]\n",
    "val_labels = [int(t['label']) for t in val]\n",
    "test_texts = [t[text_col] for t in test]\n",
    "test_labels = [int(t['label']) for t in test]\n",
    "\n",
    "# -------------------------\n",
    "# Simple tokenizer (NB, CNN, LSTM)\n",
    "# -------------------------\n",
    "def tokenize_simple(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    return re.findall(r\"\\w+|[!?.]\", str(text).lower())\n",
    "\n",
    "# -------------------------\n",
    "# MODEL 1: Manual Naive Bayes (with explicit priors & likelihoods)\n",
    "# -------------------------\n",
    "print(\"\\n--- Model 1: Manual Naive Bayes ---\")\n",
    "# build vocab\n",
    "vocab = set()\n",
    "for t in train_texts:\n",
    "    vocab.update(tokenize_simple(t))\n",
    "vocab = sorted(vocab)\n",
    "V = len(vocab)\n",
    "vocab_index = {w:i for i,w in enumerate(vocab)}\n",
    "\n",
    "wc_spam = Counter()\n",
    "wc_ham = Counter()\n",
    "spam_docs = 0\n",
    "ham_docs = 0\n",
    "\n",
    "for text, label in zip(train_texts, train_labels):\n",
    "    toks = tokenize_simple(text)\n",
    "    if label == 1:\n",
    "        spam_docs += 1\n",
    "        wc_spam.update(toks)\n",
    "    else:\n",
    "        ham_docs += 1\n",
    "        wc_ham.update(toks)\n",
    "\n",
    "total_docs = len(train_labels)\n",
    "P_spam = spam_docs / total_docs if total_docs>0 else 0.5\n",
    "P_ham = ham_docs / total_docs if total_docs>0 else 0.5\n",
    "\n",
    "total_spam_words = sum(wc_spam.values())\n",
    "total_ham_words = sum(wc_ham.values())\n",
    "\n",
    "# Laplace smoothing (alpha=1)\n",
    "alpha = 1.0\n",
    "den_spam = total_spam_words + alpha * V\n",
    "den_ham = total_ham_words + alpha * V\n",
    "\n",
    "loglik_spam = {}\n",
    "loglik_ham = {}\n",
    "\n",
    "for w in vocab:\n",
    "    loglik_spam[w] = math.log((wc_spam[w] + alpha) / den_spam)\n",
    "    loglik_ham[w]  = math.log((wc_ham[w]  + alpha) / den_ham)\n",
    "\n",
    "# UNK log-prob for words not seen in training\n",
    "log_unk_spam = math.log(alpha / den_spam)\n",
    "log_unk_ham  = math.log(alpha / den_ham)\n",
    "\n",
    "def predict_nb(text):\n",
    "    toks = tokenize_simple(text)\n",
    "    score_spam = math.log(P_spam + 1e-12)\n",
    "    score_ham = math.log(P_ham + 1e-12)\n",
    "    for t in toks:\n",
    "        score_spam += loglik_spam.get(t, log_unk_spam)\n",
    "        score_ham += loglik_ham.get(t, log_unk_ham)\n",
    "    return 1 if score_spam > score_ham else 0\n",
    "\n",
    "nb_preds = [predict_nb(t) for t in test_texts]\n",
    "print(classification_report(test_labels, nb_preds, digits=4))\n",
    "\n",
    "# Print priors and top features (weights) to show the math\n",
    "print(f\"P(spam) = {P_spam:.4f}, P(ham) = {P_ham:.4f}\")\n",
    "llr = {w: (loglik_spam[w] - loglik_ham[w]) for w in vocab}\n",
    "top_spam_words = sorted(llr.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "print(\"\\nTop words favoring 'spam' (word, log-likelihood-ratio):\")\n",
    "for w,score in top_spam_words[:20]:\n",
    "    print(f\"{w}\\t{score:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# MODEL 2: Logistic Regression (Handcrafted Features + TF-IDF)\n",
    "# -------------------------\n",
    "print(\"\\n--- Model 2: Logistic Regression (Handcrafted + TF-IDF) ---\")\n",
    "\n",
    "def extract_features(text):\n",
    "    txt = str(text)\n",
    "    return {\n",
    "        \"len\": len(txt),\n",
    "        \"exclaim\": txt.count(\"!\"),\n",
    "        \"digits\": sum(ch.isdigit() for ch in txt),\n",
    "        \"uppercase\": sum(1 for ch in txt if ch.isupper()),\n",
    "        \"urls\": len(re.findall(r\"http|www|\\\\.com\", txt.lower())),\n",
    "        \"spam_kw\": sum(1 for k in [\"free\",\"win\",\"winner\",\"cash\",\"urgent\",\"click\"] if k in txt.lower())\n",
    "    }\n",
    "\n",
    "X_train_feats = pd.DataFrame([extract_features(t) for t in train_texts])\n",
    "X_test_feats  = pd.DataFrame([extract_features(t) for t in test_texts])\n",
    "\n",
    "# TF-IDF\n",
    "tf = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "X_train_tfidf = tf.fit_transform(train_texts)\n",
    "X_test_tfidf = tf.transform(test_texts)\n",
    "\n",
    "X_train_dense = csr_matrix(X_train_feats.values)\n",
    "X_test_dense  = csr_matrix(X_test_feats.values)\n",
    "\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_dense])\n",
    "X_test_combined  = hstack([X_test_tfidf, X_test_dense])\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000, random_state=SEED, solver='saga', n_jobs=-1)\n",
    "lr.fit(X_train_combined, train_labels)\n",
    "lr_preds = lr.predict(X_test_combined)\n",
    "print(classification_report(test_labels, lr_preds, digits=4))\n",
    "\n",
    "# -------------------------\n",
    "# Prepare vocab & encoding for CNN/LSTM\n",
    "# -------------------------\n",
    "print(\"\\nPreparing vocab for CNN/LSTM...\")\n",
    "vocab2 = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "for t in train_texts:\n",
    "    for tok in tokenize_simple(t):\n",
    "        if tok not in vocab2:\n",
    "            vocab2[tok] = len(vocab2)\n",
    "\n",
    "MAX_LEN = 40\n",
    "\n",
    "def encode_text(text, max_len=MAX_LEN):\n",
    "    toks = tokenize_simple(text)\n",
    "    ids = [vocab2.get(tok, 1) for tok in toks]\n",
    "    ids = ids[:max_len]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [0] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "class TorchTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_len=MAX_LEN):\n",
    "        self.X = [encode_text(t, max_len) for t in texts]\n",
    "        self.y = labels\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(int(self.y[idx]), dtype=torch.long)\n",
    "\n",
    "BATCH = 32\n",
    "train_dl = DataLoader(TorchTextDataset(train_texts, train_labels), batch_size=BATCH, shuffle=True)\n",
    "test_dl  = DataLoader(TorchTextDataset(test_texts, test_labels), batch_size=BATCH)\n",
    "\n",
    "counts = Counter(train_labels)\n",
    "total_samples = len(train_labels)\n",
    "class_weights = torch.tensor([\n",
    "    total_samples / (2.0 * counts.get(0, 1)),\n",
    "    total_samples / (2.0 * counts.get(1, 1))\n",
    "], dtype=torch.float).to(device)\n",
    "\n",
    "# -------------------------\n",
    "# MODEL 3: TextCNN\n",
    "# -------------------------\n",
    "print(\"\\n--- Model 3: TextCNN ---\")\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, num_filters=100):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.conv3 = nn.Conv1d(embed_dim, num_filters, kernel_size=3)\n",
    "        self.conv4 = nn.Conv1d(embed_dim, num_filters, kernel_size=4)\n",
    "        self.conv5 = nn.Conv1d(embed_dim, num_filters, kernel_size=5)\n",
    "        self.fc = nn.Linear(num_filters*3, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).transpose(1,2)\n",
    "        c3 = torch.relu(self.conv3(x)).max(dim=2)[0]\n",
    "        c4 = torch.relu(self.conv4(x)).max(dim=2)[0]\n",
    "        c5 = torch.relu(self.conv5(x)).max(dim=2)[0]\n",
    "        out = torch.cat([c3,c4,c5], dim=1)\n",
    "        return self.fc(out)\n",
    "\n",
    "cnn_model = TextCNN(len(vocab2)).to(device)\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "EPOCHS = 2\n",
    "for ep in range(EPOCHS):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0.0\n",
    "    for X,y in train_dl:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = cnn_model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[CNN] Epoch {ep+1}/{EPOCHS} loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "cnn_model.eval()\n",
    "cnn_preds = []\n",
    "with torch.no_grad():\n",
    "    for X,y in test_dl:\n",
    "        X = X.to(device)\n",
    "        logits = cnn_model(X)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        cnn_preds.extend(preds.tolist())\n",
    "print(classification_report(test_labels, cnn_preds, digits=4))\n",
    "\n",
    "print(\"\\nExample conv3 filter weights (first filter):\")\n",
    "print(cnn_model.conv3.weight.data[0][:10].cpu().numpy())\n",
    "\n",
    "# -------------------------\n",
    "# MODEL 4: BiLSTM\n",
    "# -------------------------\n",
    "print(\"\\n--- Model 4: BiLSTM ---\")\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, hidden=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden*2, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        out, (h, c) = self.lstm(x)\n",
    "        final = torch.cat([h[-2], h[-1]], dim=1)\n",
    "        return self.fc(final)\n",
    "\n",
    "lstm_model = BiLSTM(len(vocab2)).to(device)\n",
    "optimizer_l = optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
    "criterion_l = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    lstm_model.train()\n",
    "    total_loss = 0.0\n",
    "    for X,y in train_dl:\n",
    "        X = X.to(device); y = y.to(device)\n",
    "        optimizer_l.zero_grad()\n",
    "        logits = lstm_model(X)\n",
    "        loss = criterion_l(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer_l.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[LSTM] Epoch {ep+1}/{EPOCHS} loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "lstm_model.eval()\n",
    "lstm_preds = []\n",
    "with torch.no_grad():\n",
    "    for X,y in test_dl:\n",
    "        X = X.to(device)\n",
    "        preds = lstm_model(X).argmax(dim=1).cpu().numpy()\n",
    "        lstm_preds.extend(preds.tolist())\n",
    "print(classification_report(test_labels, lstm_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16777d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# MANUAL INPUT PREDICTION SECTION\n",
    "# ================================\n",
    "\n",
    "def predict_manual(text):\n",
    "    print(\"\\n=== Manual Prediction ===\")\n",
    "\n",
    "    # 1) Naive Bayes prediction\n",
    "    nb_pred = predict_nb(text)\n",
    "\n",
    "    # 2) Logistic Regression TF-IDF prediction\n",
    "    X_feats = csr_matrix(pd.DataFrame([extract_features(text)]).values)\n",
    "    X_tfidf = tf.transform([text])\n",
    "    X_combined = hstack([X_tfidf, X_feats])\n",
    "    lr_pred = lr.predict(X_combined)[0]\n",
    "\n",
    "    # 3) CNN prediction\n",
    "    cnn_model.eval()\n",
    "    X_enc = torch.tensor([encode_text(text)], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        cnn_pred = cnn_model(X_enc).argmax(dim=1).item()\n",
    "\n",
    "    # 4) LSTM prediction\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_pred = lstm_model(X_enc).argmax(dim=1).item()\n",
    "\n",
    "    label_map = {0: \"HAM\", 1: \"SPAM\"}\n",
    "\n",
    "    print(f\"Naive Bayes:          {label_map[nb_pred]}\")\n",
    "    print(f\"Logistic Regression:  {label_map[lr_pred]}\")\n",
    "    print(f\"TextCNN:              {label_map[cnn_pred]}\")\n",
    "    print(f\"BiLSTM:               {label_map[lstm_pred]}\")\n",
    "\n",
    "    return lr_pred  # default model\n",
    "\n",
    "\n",
    "# ================================\n",
    "# CLI MODE (runs when script is executed)\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        msg = input(\"\\nType a message (or 'exit'): \")\n",
    "        if msg.lower() == \"exit\":\n",
    "            break\n",
    "        predict_manual(msg)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
