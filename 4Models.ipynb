{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ebc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading dataset: mshenoda/spam-messages\n",
      "Using text column: text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47392/47392 [00:00<00:00, 71031.24 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5923/5923 [00:00<00:00, 71800.60 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5926/5926 [00:00<00:00, 68002.47 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 1: Naive Bayes ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9749    0.8954    0.9334      3595\n",
      "           1     0.8567    0.9644    0.9074      2331\n",
      "\n",
      "    accuracy                         0.9225      5926\n",
      "   macro avg     0.9158    0.9299    0.9204      5926\n",
      "weighted avg     0.9284    0.9225    0.9232      5926\n",
      "\n",
      "\n",
      "=== Model 2: Logistic Regression ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5939    0.5752    0.5844      3595\n",
      "           1     0.3752    0.3934    0.3841      2331\n",
      "\n",
      "    accuracy                         0.5037      5926\n",
      "   macro avg     0.4846    0.4843    0.4843      5926\n",
      "weighted avg     0.5079    0.5037    0.5056      5926\n",
      "\n",
      "\n",
      "Preparing vocab for neural models...\n",
      "\n",
      "=== Model 3: CNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9640    0.9744    0.9692      3595\n",
      "           1     0.9599    0.9438    0.9518      2331\n",
      "\n",
      "    accuracy                         0.9624      5926\n",
      "   macro avg     0.9619    0.9591    0.9605      5926\n",
      "weighted avg     0.9623    0.9624    0.9623      5926\n",
      "\n",
      "\n",
      "=== Model 4: BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9475    0.9697    0.9585      3595\n",
      "           1     0.9515    0.9172    0.9340      2331\n",
      "\n",
      "    accuracy                         0.9490      5926\n",
      "   macro avg     0.9495    0.9434    0.9463      5926\n",
      "weighted avg     0.9491    0.9490    0.9489      5926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, math, re, warnings\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# Global Setup\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ============================================================\n",
    "# Dataset Loading + Label Normalization\n",
    "# ============================================================\n",
    "HF_DATASET = \"mshenoda/spam-messages\"\n",
    "\n",
    "def load_data():\n",
    "    print(f\"Loading dataset: {HF_DATASET}\")\n",
    "    ds = load_dataset(HF_DATASET)\n",
    "\n",
    "    cols = ds[\"train\"].column_names\n",
    "    if \"text\" in cols: text_col = \"text\"\n",
    "    elif \"message\" in cols: text_col = \"message\"\n",
    "    else:\n",
    "        text_col = next((c for c in cols if ds[\"train\"].features[c].dtype == \"string\"), cols[0])\n",
    "\n",
    "    print(\"Using text column:\", text_col)\n",
    "\n",
    "    label_map = {\"ham\": 0, \"spam\": 1, \"0\": 0, \"1\": 1}\n",
    "\n",
    "    def normalize(example):\n",
    "        lab = example.get(\"label\")\n",
    "        if isinstance(lab, (int, float)):\n",
    "            example[\"label\"] = int(lab)\n",
    "        else:\n",
    "            example[\"label\"] = label_map.get(str(lab).lower(), 0)\n",
    "        return example\n",
    "\n",
    "    ds = ds.map(normalize)\n",
    "\n",
    "    if \"validation\" in ds:\n",
    "        train, val, test = ds[\"train\"], ds[\"validation\"], ds[\"test\"]\n",
    "    else:\n",
    "        split = ds[\"train\"].train_test_split(test_size=0.2, seed=SEED)\n",
    "        train, val, test = split[\"train\"], split[\"test\"], split[\"test\"]\n",
    "\n",
    "    return (\n",
    "        [x[text_col] for x in train], [int(x[\"label\"]) for x in train],\n",
    "        [x[text_col] for x in val],   [int(x[\"label\"]) for x in val],\n",
    "        [x[text_col] for x in test],  [int(x[\"label\"]) for x in test]\n",
    "    )\n",
    "\n",
    "train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = load_data()\n",
    "\n",
    "# ============================================================\n",
    "# Tokenizer / Utilities\n",
    "# ============================================================\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w+|[!?.]\", str(text).lower())\n",
    "\n",
    "# ============================================================\n",
    "# Model 1: Naive Bayes\n",
    "# ============================================================\n",
    "def train_naive_bayes(texts, labels):\n",
    "    vocab = set()\n",
    "    for t in texts:\n",
    "        vocab.update(tokenize(t))\n",
    "    vocab = sorted(vocab)\n",
    "\n",
    "    wc_spam = Counter()\n",
    "    wc_ham = Counter()\n",
    "\n",
    "    spam_docs = sum(1 for l in labels if l == 1)\n",
    "    ham_docs = len(labels) - spam_docs\n",
    "    total_docs = len(labels)\n",
    "\n",
    "    for txt, lab in zip(texts, labels):\n",
    "        toks = tokenize(txt)\n",
    "        (wc_spam if lab == 1 else wc_ham).update(toks)\n",
    "\n",
    "    P_spam = spam_docs / total_docs\n",
    "    P_ham = ham_docs / total_docs\n",
    "\n",
    "    alpha = 1\n",
    "    V = len(vocab)\n",
    "\n",
    "    total_spam = sum(wc_spam.values()) + alpha * V\n",
    "    total_ham  = sum(wc_ham.values()) + alpha * V\n",
    "\n",
    "    log_spam = {w: math.log((wc_spam[w] + alpha) / total_spam) for w in vocab}\n",
    "    log_ham  = {w: math.log((wc_ham[w]  + alpha) / total_ham) for w in vocab}\n",
    "\n",
    "    unk_spam = math.log(alpha / total_spam)\n",
    "    unk_ham = math.log(alpha / total_ham)\n",
    "\n",
    "    def predict(text):\n",
    "        toks = tokenize(text)\n",
    "        s_spam = math.log(P_spam + 1e-12)\n",
    "        s_ham  = math.log(P_ham + 1e-12)\n",
    "        for t in toks:\n",
    "            s_spam += log_spam.get(t, unk_spam)\n",
    "            s_ham  += log_ham.get(t,  unk_ham)\n",
    "        return 1 if s_spam > s_ham else 0\n",
    "\n",
    "    return predict\n",
    "\n",
    "print(\"\\n=== Model 1: Naive Bayes ===\")\n",
    "nb_model = train_naive_bayes(train_texts, train_labels)\n",
    "nb_preds = [nb_model(t) for t in test_texts]\n",
    "print(classification_report(test_labels, nb_preds, digits=4))\n",
    "\n",
    "# ============================================================\n",
    "# Model 2: Logistic Regression\n",
    "# ============================================================\n",
    "def extract_features(txt):\n",
    "    t = str(txt)\n",
    "    return {\n",
    "        \"len\": len(t),\n",
    "        \"exclaim\": t.count(\"!\"),\n",
    "        \"digits\": sum(c.isdigit() for c in t),\n",
    "        \"upper\": sum(c.isupper() for c in t),\n",
    "        \"urls\": len(re.findall(r\"http|www|\\\\.com\", t.lower())),\n",
    "        \"spam_kw\": sum(k in t.lower() for k in [\"free\", \"win\", \"cash\", \"urgent\", \"click\"])\n",
    "    }\n",
    "\n",
    "print(\"\\n=== Model 2: Logistic Regression ===\")\n",
    "\n",
    "df_train = pd.DataFrame([extract_features(t) for t in train_texts])\n",
    "df_test  = pd.DataFrame([extract_features(t) for t in test_texts])\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "X_train_tfidf = tfidf.fit_transform(train_texts)\n",
    "X_test_tfidf  = tfidf.transform(test_texts)\n",
    "\n",
    "X_train = hstack([X_train_tfidf, csr_matrix(df_train.values)])\n",
    "X_test  = hstack([X_test_tfidf,  csr_matrix(df_test.values)])\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000, solver=\"saga\", n_jobs=-1)\n",
    "lr.fit(X_train, train_labels)\n",
    "lr_preds = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(test_labels, lr_preds, digits=4))\n",
    "\n",
    "# ============================================================\n",
    "# Shared Embedding Vocab (CNN & LSTM)\n",
    "# ============================================================\n",
    "print(\"\\nPreparing vocab for neural models...\")\n",
    "\n",
    "vocab = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "for t in train_texts:\n",
    "    for tok in tokenize(t):\n",
    "        if tok not in vocab:\n",
    "            vocab[tok] = len(vocab)\n",
    "\n",
    "MAX_LEN = 40\n",
    "\n",
    "def encode(text):\n",
    "    toks = tokenize(text)\n",
    "    ids = [vocab.get(t, 1) for t in toks[:MAX_LEN]]\n",
    "    return ids + [0]*(MAX_LEN - len(ids))\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.X = [encode(t) for t in texts]\n",
    "        self.y = labels\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx]), torch.tensor(self.y[idx])\n",
    "\n",
    "train_dl = DataLoader(TextDataset(train_texts, train_labels), batch_size=32, shuffle=True)\n",
    "test_dl  = DataLoader(TextDataset(test_texts, test_labels),  batch_size=32)\n",
    "\n",
    "# ============================================================\n",
    "# Model 3: CNN\n",
    "# ============================================================\n",
    "print(\"\\n=== Model 3: CNN ===\")\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, 100, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(100, 100, 3),\n",
    "            nn.Conv1d(100, 100, 4),\n",
    "            nn.Conv1d(100, 100, 5)\n",
    "        ])\n",
    "        self.fc = nn.Linear(300, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).transpose(1,2)\n",
    "        pools = [torch.relu(c(x)).max(2)[0] for c in self.convs]\n",
    "        return self.fc(torch.cat(pools, 1))\n",
    "\n",
    "cnn = TextCNN(len(vocab)).to(device)\n",
    "opt = optim.Adam(cnn.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for _ in range(2):\n",
    "    cnn.train()\n",
    "    for X, y in train_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = crit(cnn(X), y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "cnn.eval()\n",
    "cnn_preds = []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dl:\n",
    "        preds = cnn(X.to(device)).argmax(1).cpu().tolist()\n",
    "        cnn_preds += preds\n",
    "\n",
    "print(classification_report(test_labels, cnn_preds, digits=4))\n",
    "\n",
    "# ============================================================\n",
    "# Model 4: BiLSTM\n",
    "# ============================================================\n",
    "print(\"\\n=== Model 4: BiLSTM ===\")\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, 100)\n",
    "        self.lstm = nn.LSTM(100, 128, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h_final = torch.cat([h[-2], h[-1]], 1)\n",
    "        return self.fc(h_final)\n",
    "\n",
    "lstm = BiLSTM(len(vocab)).to(device)\n",
    "opt_l = optim.Adam(lstm.parameters(), lr=1e-3)\n",
    "\n",
    "for _ in range(2):\n",
    "    lstm.train()\n",
    "    for X, y in train_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        opt_l.zero_grad()\n",
    "        loss = crit(lstm(X), y)\n",
    "        loss.backward()\n",
    "        opt_l.step()\n",
    "\n",
    "lstm.eval()\n",
    "lstm_preds = []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dl:\n",
    "        lstm_preds += lstm(X.to(device)).argmax(1).cpu().tolist()\n",
    "\n",
    "print(classification_report(test_labels, lstm_preds, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca80d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "print(\"Hello from Streamlit!\")\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ==========================================================\n",
    "#                  BASIC UTILITIES\n",
    "# ==========================================================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w+|[!?.]\", str(text).lower())\n",
    "\n",
    "def softmax(logits):\n",
    "    a = np.array(logits)\n",
    "    e = np.exp(a - np.max(a))\n",
    "    return e / e.sum()\n",
    "\n",
    "# ==========================================================\n",
    "#                  LOAD DATASET\n",
    "# ==========================================================\n",
    "@st.cache_resource\n",
    "def load_data():\n",
    "    ds = load_dataset(\"mshenoda/spam-messages\")\n",
    "    cols = ds[\"train\"].column_names\n",
    "\n",
    "    text_col = \"text\" if \"text\" in cols else cols[0]\n",
    "    label_map = {\"ham\": 0, \"spam\": 1, \"0\": 0, \"1\": 1}\n",
    "\n",
    "    def normalize(example):\n",
    "        lab = example.get(\"label\")\n",
    "        if isinstance(lab, (int, float)):\n",
    "            example[\"label\"] = int(lab)\n",
    "        else:\n",
    "            example[\"label\"] = label_map.get(str(lab).lower(), 0)\n",
    "        return example\n",
    "\n",
    "    ds = ds.map(normalize)\n",
    "\n",
    "    split = ds[\"train\"].train_test_split(test_size=0.2, seed=SEED)\n",
    "    train, test = split[\"train\"], split[\"test\"]\n",
    "\n",
    "    return (\n",
    "        [x[text_col] for x in train],\n",
    "        [int(x[\"label\"]) for x in train],\n",
    "        [x[text_col] for x in test],\n",
    "        [int(x[\"label\"]) for x in test]\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "#                 NAIVE BAYES IMPLEMENTATION\n",
    "# ==========================================================\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.vocab = None\n",
    "        self.log_spam = {}\n",
    "        self.log_ham = {}\n",
    "        self.P_spam = 0.5\n",
    "        self.P_ham = 0.5\n",
    "        self.unk_spam = 0.0\n",
    "        self.unk_ham = 0.0\n",
    "\n",
    "    def fit(self, texts, labels, alpha=1):\n",
    "        vocab = set()\n",
    "        for t in texts: vocab.update(tokenize(t))\n",
    "        vocab = sorted(vocab)\n",
    "        wc_spam, wc_ham = Counter(), Counter()\n",
    "\n",
    "        spam_docs = sum(1 for l in labels if l == 1)\n",
    "        ham_docs  = len(labels) - spam_docs\n",
    "        total_docs = len(labels)\n",
    "\n",
    "        for txt, lab in zip(texts, labels):\n",
    "            toks = tokenize(txt)\n",
    "            (wc_spam if lab == 1 else wc_ham).update(toks)\n",
    "\n",
    "        self.P_spam = spam_docs / total_docs\n",
    "        self.P_ham  = ham_docs  / total_docs\n",
    "\n",
    "        V = len(vocab)\n",
    "        total_spam = sum(wc_spam.values()) + alpha * V\n",
    "        total_ham  = sum(wc_ham.values()) + alpha * V\n",
    "\n",
    "        self.log_spam = {w: math.log((wc_spam[w] + alpha) / total_spam) for w in vocab}\n",
    "        self.log_ham  = {w: math.log((wc_ham[w] + alpha) / total_ham)  for w in vocab}\n",
    "\n",
    "        self.unk_spam = math.log(alpha / total_spam)\n",
    "        self.unk_ham  = math.log(alpha / total_ham)\n",
    "        self.vocab = set(vocab)\n",
    "\n",
    "    def predict_proba(self, text):\n",
    "        toks = tokenize(text)\n",
    "        s_spam = math.log(self.P_spam + 1e-12)\n",
    "        s_ham  = math.log(self.P_ham  + 1e-12)\n",
    "\n",
    "        for t in toks:\n",
    "            s_spam += self.log_spam.get(t, self.unk_spam)\n",
    "            s_ham  += self.log_ham.get(t,  self.unk_ham)\n",
    "\n",
    "        probs = softmax([s_ham, s_spam])\n",
    "        return probs\n",
    "\n",
    "# ==========================================================\n",
    "#                   CNN MODEL\n",
    "# ==========================================================\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, 100, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(100, 100, 3),\n",
    "            nn.Conv1d(100, 100, 4),\n",
    "            nn.Conv1d(100, 100, 5)\n",
    "        ])\n",
    "        self.fc = nn.Linear(300, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).transpose(1,2)\n",
    "        pools = [torch.relu(c(x)).max(2)[0] for c in self.convs]\n",
    "        return self.fc(torch.cat(pools, 1))\n",
    "\n",
    "# ==========================================================\n",
    "#                   BiLSTM MODEL\n",
    "# ==========================================================\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, hidden=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden * 2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h = torch.cat([h[0], h[1]], dim=1)\n",
    "        return self.fc(h)\n",
    "\n",
    "# ==========================================================\n",
    "#         VOCAB + ENCODING FUNCTION\n",
    "# ==========================================================\n",
    "@st.cache_resource\n",
    "def build_vocab(texts):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for t in texts:\n",
    "        for tok in tokenize(t):\n",
    "            if tok not in vocab:\n",
    "                vocab[tok] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "MAX_LEN = 40\n",
    "\n",
    "def encode(text, vocab):\n",
    "    toks = tokenize(text)\n",
    "    ids = [vocab.get(t, 1) for t in toks[:MAX_LEN]]\n",
    "    return ids + [0] * (MAX_LEN - len(ids))\n",
    "\n",
    "# ==========================================================\n",
    "#                 TRAIN ALL MODELS\n",
    "# ==========================================================\n",
    "@st.cache_resource\n",
    "def train_all_models():\n",
    "    train_texts, train_labels, _, _ = load_data()\n",
    "\n",
    "    # ---- NB ----\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(train_texts, train_labels)\n",
    "\n",
    "    # ---- LR ----\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "    X_train_tfidf = tfidf.fit_transform(train_texts)\n",
    "\n",
    "    extra = np.array([[\n",
    "        len(t), t.count(\"!\"), sum(c.isdigit() for c in t),\n",
    "        sum(c.isupper() for c in t),\n",
    "        len(re.findall(r\"http|www|\\.com\", t.lower())),\n",
    "        sum(k in t.lower() for k in [\"free\", \"win\", \"cash\", \"urgent\", \"click\"])\n",
    "    ] for t in train_texts])\n",
    "\n",
    "    X_lr = hstack([X_train_tfidf, csr_matrix(extra)])\n",
    "    lr = LogisticRegression(max_iter=2000, solver=\"saga\", n_jobs=-1)\n",
    "    lr.fit(X_lr, train_labels)\n",
    "\n",
    "    # ---- Vocab ----\n",
    "    vocab = build_vocab(train_texts)\n",
    "\n",
    "    # ---- CNN ----\n",
    "    cnn = TextCNN(len(vocab)).to(device)\n",
    "    cnn_opt = optim.Adam(cnn.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    encoded = [encode(t, vocab) for t in train_texts]\n",
    "    ds = list(zip(encoded, train_labels))\n",
    "\n",
    "    cnn.train()\n",
    "    for epoch in range(1):\n",
    "        np.random.shuffle(ds)\n",
    "        for i in range(0, len(ds), 64):\n",
    "            batch = ds[i:i+64]\n",
    "            Xb = torch.tensor([b[0] for b in batch], dtype=torch.long).to(device)\n",
    "            yb = torch.tensor([b[1] for b in batch], dtype=torch.long).to(device)\n",
    "\n",
    "            cnn_opt.zero_grad()\n",
    "            loss = loss_fn(cnn(Xb), yb)\n",
    "            loss.backward()\n",
    "            cnn_opt.step()\n",
    "    cnn.eval()\n",
    "\n",
    "    # ---- BiLSTM ----\n",
    "    lstm = BiLSTM(len(vocab)).to(device)\n",
    "    lstm_opt = optim.Adam(lstm.parameters(), lr=1e-3)\n",
    "\n",
    "    lstm.train()\n",
    "    for epoch in range(1):\n",
    "        np.random.shuffle(ds)\n",
    "        for i in range(0, len(ds), 64):\n",
    "            batch = ds[i:i+64]\n",
    "            Xb = torch.tensor([b[0] for b in batch], dtype=torch.long).to(device)\n",
    "            yb = torch.tensor([b[1] for b in batch], dtype=torch.long).to(device)\n",
    "            lstm_opt.zero_grad()\n",
    "            loss = loss_fn(lstm(Xb), yb)\n",
    "            loss.backward()\n",
    "            lstm_opt.step()\n",
    "    lstm.eval()\n",
    "\n",
    "    return {\n",
    "        \"nb\": nb,\n",
    "        \"lr\": lr,\n",
    "        \"tfidf\": tfidf,\n",
    "        \"cnn\": cnn,\n",
    "        \"lstm\": lstm,\n",
    "        \"vocab\": vocab\n",
    "    }\n",
    "\n",
    "models = train_all_models()\n",
    "\n",
    "# ==========================================================\n",
    "#           PREDICT ACROSS ALL FOUR MODELS\n",
    "# ==========================================================\n",
    "def predict_all_models(text):\n",
    "    results = []\n",
    "    vocab = models[\"vocab\"]\n",
    "\n",
    "    # NB\n",
    "    probs = models[\"nb\"].predict_proba(text)\n",
    "    results.append((\"Naive Bayes\", int(np.argmax(probs)), float(max(probs)), probs.tolist()))\n",
    "\n",
    "    # LR\n",
    "    tfidf = models[\"tfidf\"]\n",
    "    lr = models[\"lr\"]\n",
    "    feat = tfidf.transform([text])\n",
    "    extra = np.array([[len(text), text.count(\"!\"), sum(c.isdigit() for c in text), sum(c.isupper() for c in text), len(re.findall(r\"http|www|\\.com\", text.lower())), sum(k in text.lower() for k in [\"free\", \"win\", \"cash\", \"urgent\", \"click\"])]])\n",
    "    Xlr = hstack([feat, csr_matrix(extra)])\n",
    "    probs = lr.predict_proba(Xlr)[0]\n",
    "    results.append((\"Logistic Regression\", int(np.argmax(probs)), float(max(probs)), probs.tolist()))\n",
    "\n",
    "    # CNN\n",
    "    ids = torch.tensor([encode(text, vocab)], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = models[\"cnn\"](ids).cpu().numpy()[0]\n",
    "        probs = softmax(logits)\n",
    "    results.append((\"CNN\", int(np.argmax(probs)), float(max(probs)), probs.tolist()))\n",
    "\n",
    "    # LSTM\n",
    "    with torch.no_grad():\n",
    "        logits = models[\"lstm\"](ids).cpu().numpy()[0]\n",
    "        probs = softmax(logits)\n",
    "    results.append((\"BiLSTM\", int(np.argmax(probs)), float(max(probs)), probs.tolist()))\n",
    "\n",
    "    return results\n",
    "\n",
    "# ==========================================================\n",
    "#                  STREAMLIT UI\n",
    "# ==========================================================\n",
    "\n",
    "st.set_page_config(page_title=\"AI Spam Detector\", page_icon=\"üì©\", layout=\"centered\")\n",
    "\n",
    "# Styling\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    ".main-title { font-size: 42px; font-weight: 900; text-align: center; color: #4A90E2; }\n",
    ".sub-text { text-align: center; font-size: 18px; color: #555; }\n",
    ".result-box { padding: 20px; border-radius: 12px; background: #f7f9fc; border: 1px solid #d8e3f0; margin-bottom: 20px; }\n",
    ".winner { padding: 25px; border-radius: 15px; background: #E3F7E0; border: 2px solid #9ED89E; }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "st.markdown('<div class=\"main-title\">üì© AI Spam Detection Demo</div>', unsafe_allow_html=True)\n",
    "st.markdown('<div class=\"sub-text\">Test one message across all four models and pick the most confident one.</div>', unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"---\")\n",
    "\n",
    "text = st.text_area(\"‚úçÔ∏è Enter a message to classify:\", height=160)\n",
    "\n",
    "if st.button(\"üöÄ Classify Message\", use_container_width=True):\n",
    "    if not text.strip():\n",
    "        st.warning(\"Please type a message first.\")\n",
    "    else:\n",
    "        with st.spinner(\"Running all models...\"):\n",
    "            results = predict_all_models(text)\n",
    "\n",
    "        st.subheader(\"üìä Model Results\")\n",
    "\n",
    "        for name, pred, conf, probs in results:\n",
    "            st.markdown(f\"<div class='result-box'>\", unsafe_allow_html=True)\n",
    "            st.markdown(f\"### üîπ {name}\")\n",
    "            st.write(f\"Prediction: **{pred}**  (0=ham, 1=spam)\")\n",
    "            st.write(f\"Confidence: **{conf:.4f}**\")\n",
    "            st.write(f\"Probabilities ‚Üí Ham: {probs[0]:.4f} | Spam: {probs[1]:.4f}\")\n",
    "            st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "        best = max(results, key=lambda x: x[2])\n",
    "\n",
    "        st.markdown(\"<div class='winner'>\", unsafe_allow_html=True)\n",
    "        st.markdown(f\"## üèÜ Best Model: **{best[0]}**\")\n",
    "        st.write(f\"Final Prediction: **{best[1]}**\")\n",
    "        st.write(f\"Highest Confidence: **{best[2]:.4f}**\")\n",
    "        st.markdown(\"</div>\", unsafe_allow_html=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
